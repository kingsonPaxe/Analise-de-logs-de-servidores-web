# ğŸ“Š AnÃ¡lise de Logs de Servidor Web

Um projeto completo de anÃ¡lise e visualizaÃ§Ã£o de logs de servidor web, utilizando tÃ©cnicas de ETL (Extract, Transform, Load) para processar, enriquecer e analisar dados de acesso a servidores.

## ğŸ“‹ DescriÃ§Ã£o do Projeto

Este projeto implementa um pipeline de dados que coleta logs de acesso HTTP bruto, realiza limpeza e enriquecimento dos dados utilizando geolocalizaÃ§Ã£o e anÃ¡lise de user-agents, e disponibiliza as informaÃ§Ãµes atravÃ©s de um dashboard interativo com Streamlit.

**Principais funcionalidades:**
- âœ… ExtraÃ§Ã£o e parsing de logs de servidor (HTTP)
- âœ… Limpeza e validaÃ§Ã£o de dados
- âœ… Enriquecimento com geolocalizaÃ§Ã£o e informaÃ§Ãµes de navegador
- âœ… Armazenamento em mÃºltiplos formatos (Parquet, SQLite, Pickle)
- âœ… Dashboard interativo com mÃ©tricas e visualizaÃ§Ãµes em Plotly
- âœ… AnÃ¡lise exploratÃ³ria com Jupyter Notebooks

---

## ğŸ“ Estrutura do Projeto

```
Analise de logs de servidor web/
â”œâ”€â”€ accessLog.csv              # Logs brutos de acesso HTTP
â”œâ”€â”€ Ips.csv                    # Dados enriquecidos de geolocalizaÃ§Ã£o de IPs
â”œâ”€â”€ access.log                 # Arquivo de log original do servidor
â”œâ”€â”€ log_dw.pkl                 # Dataset processado (pickle format)
â”œâ”€â”€ logServidores_web.db       # Banco de dados SQLite com dados processados
â”œâ”€â”€ AnÃ¡lise.ipynb              # Notebook com anÃ¡lise exploratÃ³ria de dados
â”œâ”€â”€ index.ipynb                # Notebook adicional para anÃ¡lises
â”œâ”€â”€ dashboard.py               # Dashboard interativo com Streamlit
â”œâ”€â”€ workflow.excalidraw        # Diagrama visual do pipeline de dados
â”œâ”€â”€ requirements.txt           # DependÃªncias Python
â””â”€â”€ README.md                  # Este arquivo
```

---

## ğŸ”„ Workflow do Projeto

```mermaid
graph LR
    A["ğŸ“ Raw Data<br/>access.log"] --> B["ğŸ Python Script<br/>Coleta e ValidaÃ§Ã£o"]
    B --> C["ğŸ”§ Extract<br/>Regex & Pandas<br/>Parsing"]
    C --> D["âš™ï¸ Transform<br/>Limpeza & Enriquecimento<br/>GeolocalizaÃ§Ã£o"]
    D --> E["ğŸ’¾ Load<br/>Armazenamento"]
    E --> F["ğŸ“Š Dashboard<br/>Streamlit App<br/>GrÃ¡ficos & MÃ©tricas"]
    
    E --> G["Parquet"]
    E --> H["SQLite3"]
    E --> I["Pickle"]
```

### Etapas do Pipeline:

1. **Extract** ğŸ“¥
   - Leitura de logs brutos em formato HTTP
   - Parsing com expressÃµes regulares
   - ValidaÃ§Ã£o de campos

2. **Transform** ğŸ”„
   - Limpeza e normalizaÃ§Ã£o de dados
   - IdentificaÃ§Ã£o de user-agents (navegadores, bots)
   - DetecÃ§Ã£o de dispositivos (mobile, tablet, PC)
   - Enriquecimento com geolocalizaÃ§Ã£o de IPs
   - CÃ¡lculo de status codes e taxa de erros

3. **Load** ğŸ’¾
   - Armazenamento em Parquet (formato colunar)
   - PersistÃªncia em SQLite3 para consultas SQL
   - ExportaÃ§Ã£o em Pickle para anÃ¡lises rÃ¡pidas

4. **VisualizaÃ§Ã£o** ğŸ“ˆ
   - Dashboard interativo com Streamlit
   - GrÃ¡ficos interativos com Plotly
   - Filtros por perÃ­odo, IP, URL e status

---

## ğŸ“Š Dados DisponÃ­veis

### Arquivos de Dados

| Arquivo | DescriÃ§Ã£o | Registros |
|---------|-----------|-----------|
| `accessLog.csv` | Logs completos com anÃ¡lise de user-agents | ~100k+ registros |
| `Ips.csv` | GeolocalizaÃ§Ã£o e informaÃ§Ãµes de rede de IPs | Dados enriquecidos |
| `log_dw.pkl` | Dataset processado em formato Pickle | Dados limpos |

### Dados ExtraÃ­dos

**Do arquivo de log cada requisiÃ§Ã£o contÃ©m:**
- IP do cliente
- Data e hora
- MÃ©todo HTTP (GET, POST, etc.)
- URL acessada
- Protocolo (HTTP/1.1, HTTP/2, etc.)
- Status HTTP
- Device type (mobile, tablet, PC)
- Bot detection
- Navegador e SO
- GeolocalizaÃ§Ã£o (paÃ­s, cidade, coordenadas)
- ISP e provedor de rede

### KPIs Principais

- **Total de requisiÃ§Ãµes:** Contagem geral de acessos
- **IPs Ãºnicos:** NÃºmero de clientes distintos
- **Taxa de erros (4xx/5xx):** Percentual de requisiÃ§Ãµes com problemas
- **URL mais visitada:** Endpoint mais acessado
- **DistribuiÃ§Ã£o de status codes:** AnÃ¡lise de sucesso vs erros
- **RequisiÃ§Ãµes por horÃ¡rio:** PadrÃ£o de acesso ao longo do dia
- **RequisiÃ§Ãµes por paÃ­s:** DistribuiÃ§Ã£o geogrÃ¡fica
- **ProporÃ§Ã£o de bots:** Percentual de acesso automatizado

---

## ğŸ› ï¸ Tecnologias e DependÃªncias

### Stack TecnolÃ³gico

| Tecnologia | VersÃ£o | PropÃ³sito |
|-----------|--------|----------|
| **Python** | 3.x | Linguagem principal |
| **Pandas** | 3.0.1 | ManipulaÃ§Ã£o de dados |
| **NumPy** | 2.4.2 | ComputaÃ§Ãµes numÃ©ricas |
| **Plotly** | 6.5.2 | VisualizaÃ§Ãµes interativas |
| **Streamlit** | - | Framework para dashboard |
| **SQLAlchemy** | 2.0.46 | ORM para banco de dados |
| **Jupyter** | 9.10.0 | Notebooks interativos |
| **user-agents** | 2.2.0 | Parsing de user-agents |
| **ua-parser** | 1.0.1 | Parser de navegadores e SO |

**Todas as dependÃªncias podem ser instaladas com:**

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Como Usar

### 1. InstalaÃ§Ã£o

```bash
# Clonar o repositÃ³rio
git clone <repositorio>
cd "Analise de logs de servidor web"

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. AnÃ¡lise ExploratÃ³ria (Jupyter)

Para analisar os dados interativamente:

```bash
jupyter notebook AnÃ¡lise.ipynb
```

O notebook `AnÃ¡lise.ipynb` contÃ©m:
- Carregamento e exploraÃ§Ã£o dos dados
- KPIs principais
- VisualizaÃ§Ãµes com Plotly
- AnÃ¡lise de tendÃªncias por horÃ¡rio
- DistribuiÃ§Ã£o geogrÃ¡fica
- SegmentaÃ§Ã£o de URLs
- DetecÃ§Ã£o de padrÃµes de acesso

### 3. Dashboard Interativo (Streamlit)

Para visualizar o dashboard em tempo real:

```bash
streamlit run dashboard.py
```

O dashboard oferece:
- ğŸ“Š VisualizaÃ§Ãµes interativas
- ğŸ” Filtros dinÃ¢micos
- ğŸ“ˆ GrÃ¡ficos de tendÃªncias
- ğŸŒ Mapa geogrÃ¡fico
- ğŸ“± AnÃ¡lise de dispositivos
- ğŸ¤– EstatÃ­sticas de bots

---

## ğŸ“ˆ CaracterÃ­sticas da AnÃ¡lise

### VisualizaÃ§Ãµes IncluÃ­das

1. **DistribuiÃ§Ã£o de Status Codes**
   - GrÃ¡fico de pizza com sucesso vs erros
   - AnÃ¡lise detalhada de 4xx e 5xx

2. **RequisiÃ§Ãµes por HorÃ¡rio**
   - Linha do tempo mostrando picos de acesso
   - IdentificaÃ§Ã£o de perÃ­odos de maior atividade

3. **Top URLs Acessadas**
   - Ranking dos endpoints mais visitados
   - Volume de requisiÃ§Ãµes por URL

4. **DistribuiÃ§Ã£o GeogrÃ¡fica**
   - Mapa dos paÃ­ses de origem
   - Tabela com IPs Ãºnicos por localizaÃ§Ã£o

5. **AnÃ¡lise de Dispositivos**
   - ProporÃ§Ã£o de mobile, tablet e desktop
   - TendÃªncias de acesso por tipo de dispositivo

6. **Atividade de Bots**
   - Percentual de acesso de bots vs usuÃ¡rios reais
   - IdentificaÃ§Ã£o de bots mais comuns

7. **AnÃ¡lise de Navegadores**
   - Navegadores mais utilizados
   - Sistemas operacionais

---

## ğŸ“ Estrutura dos Notebooks

### `AnÃ¡lise.ipynb`
Notebook principal com anÃ¡lise completa dos dados:
- **SeÃ§Ã£o 1:** Carregamento e informaÃ§Ãµes dos dados
- **SeÃ§Ã£o 2:** KPIs principais (total de requisiÃ§Ãµes, IPs Ãºnicos, taxa de erros)
- **SeÃ§Ã£o 3:** AnÃ¡lise temporal (padrÃµes por hora)
- **SeÃ§Ã£o 4:** GeolocalizaÃ§Ã£o e origem dos acessos
- **SeÃ§Ã£o 5:** AnÃ¡lise de URLs e endpoints
- **SeÃ§Ã£o 6:** SegmentaÃ§Ã£o por dispositivo
- **SeÃ§Ã£o 7:** DetecÃ§Ã£o e anÃ¡lise de bots

### `index.ipynb`
Notebook adicional com anÃ¡lises complementares e experimentaÃ§Ãµes.

---

## ğŸ’¡ Insights PossÃ­veis

Com esta anÃ¡lise vocÃª pode:

- ğŸ” Identificar padrÃµes de acesso ao servidor
- ğŸŒ Descobrir origem geogrÃ¡fica do trÃ¡fego
- ğŸ¤– Detectar atividade de bots e crawlers
- ğŸ“± Entender quais dispositivos mais acessam o site
- â° Otimizar infraestrutura baseado em horÃ¡rios de pico
- ğŸ›¡ï¸ Identificar tentativas de acesso indevido (4xx/5xx)
- ğŸ’¼ Gerar relatÃ³rios executivos de trÃ¡fego
- ğŸ“Š Comparar performance entre URLs

---

## ğŸ“¦ Formatos de Armazenamento

O projeto utiliza mÃºltiplos formatos para diferentes necessidades:

### Pickle (`.pkl`)
- **Uso:** Carregamento rÃ¡pido para anÃ¡lises em Jupyter
- **Vantagem:** Preserva tipos de dados Python, carregamento instantÃ¢neo
- **Arquivo:** `log_dw.pkl`

### SQLite (`.db`)
- **Uso:** Consultas SQL, relatÃ³rios dinÃ¢micos
- **Vantagem:** Portabilidade, suporta queries complexas
- **Arquivo:** `logServidores_web.db`

### Parquet
- **Uso:** Armazenamento comprimido, analytics
- **Vantagem:** CompressÃ£o, leitura coluna-por-coluna eficiente
- **Ideal para:** Big data e anÃ¡lises estatÃ­sticas

---

## ğŸ”§ CustomizaÃ§Ãµes PossÃ­veis

VocÃª pode adaptar o projeto para:

- Adicionar novos filtros no dashboard
- Incluir anÃ¡lise de performance (response time)
- Integrar alertas para anomalias
- Expandir anÃ¡lise de seguranÃ§a
- Gerar relatÃ³rios automÃ¡ticos
- Adicionar machine learning para previsÃµes
- Integrar com ferramentas de monitoramento

---

## ğŸ“‹ Requisitos do Sistema

- **Python:** 3.8+
- **MemÃ³ria RAM:** 2GB (mÃ­nimo para anÃ¡lise completa)
- **EspaÃ§o em disco:** 500MB para dados + dependÃªncias
- **Navegador:** Qualquer navegador moderno para o dashboard

---

## ğŸ“§ InformaÃ§Ãµes Adicionais

Este projeto foi desenvolvido como parte de um estÃ¡gio, com foco em:
- Engenharia de dados (ETL pipeline)
- AnÃ¡lise exploratÃ³ria de dados (EDA)
- VisualizaÃ§Ã£o de dados
- Desenvolvimento de dashboards

---

## ğŸ“„ Archivos Especiais

- **`workflow.excalidraw`:** Diagrama visual do pipeline (abrir com Excalidraw)
- **`tabela operadores re.pdf`:** ReferÃªncia de expressÃµes regulares usadas na parsing

---

## âœ¨ Status do Projeto

- âœ… Pipeline ETL completo
- âœ… Dashboard funcional
- âœ… AnÃ¡lise exploratÃ³ria
- âœ… DocumentaÃ§Ã£o
- ğŸ”„ PossÃ­veis melhorias e expansÃµes

---

**Desenvolvido com â¤ï¸ durante estÃ¡gio**

*Ãšltima atualizaÃ§Ã£o: Fevereiro de 2026*
